---
title: "Comparaison FROGS/MOTHUR/UPARSE sur la banque silva"
author: "Mahendra Mariadassou, Géraldine Pascal"
date: "17 juin 2016"
output:
  html_document:
    number_sections: yes
    theme: united
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
## setwd("~/Local/Research_Projects/Microbiota/Geraldine/")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, fig.path = "Figures_20160617/Fig")
```

# Introduction 

Ce document R markdown a vocation à documenter la procédure suivie pour comparer les résultats de FROGS, Mothur et UParse sur les jeux de données simulées. En particulier, on cherche à savoir si Frogs permet d'avoir une vision taxonomique des communautés bactériennes plus fidèle que Mothur et UParse. 

# Import et préparation des données

## Chargement des packages

On commence par charger quelques packages utiles:
- reshape et dplyr pour la manipulation de données
- ggplot2, grid et gridExtra pour la visualisation
- PMCMR pour effectuer des tests de Dunn (version non paramétrique du test post-hoc de Tukey)

```{r load-packages}
library(ggplot2)
library(grid)
library(gridExtra)
library(reshape2)
library(PMCMR) ## The Pairwise Multiple Comparison of Mean Ranks Package (PMCMR)
library(multcomp)
library(dplyr)
```

et une fonction utilitaire pour combiner plusieurs graphiques ggplot:
```{r utility-functions}
## http://stackoverflow.com/questions/13649473/add-a-common-legend-for-combined-ggplots
grid_arrange_shared_legend <- function(..., ncol = 1) {
  require(grid)
    plots <- list(...)
    g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
    legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
    lheight <- sum(legend$height)
    grid.arrange(
        do.call(arrangeGrob, c(lapply(plots, function(x)
            x + theme(legend.position="none")), ncol = ncol)),
        legend,
        nrow = 2,
        heights = unit.c(unit(1, "npc") - lheight, lheight))
}
```


## Import des données

On importe ensuite les données
```{r import-data}
data.silva <- read.table("silva.tsv", sep = "\t", header = TRUE)
head(data.silva)
```

## Transformation des données

Les différentes réponses d'intéret sont `r names(data.silva)[-c(1:7)]` stockées sur plusieurs colonnes. Avant toute chose, on change la variale nb_OTU en une variable numérique (pour retrouver un ordonnancement naturel)

```{r change-nb_OTU}
data.silva$nb_OTU <- as.numeric(sub("sp", "", data.silva$nb_OTU))
```


Dans notre contexte on cherche à comparer les performances de mothur et silva à celles de frogs en termes de divergence à plusieurs niveaux taxonomiques. On va donc réorganiser le tableau en considérant que chaque "unité expérimentale"" est un jeu de données décrit à un niveau taxonomique donné par une méthode donnée et qu'on mesure dessus la divergence entre compositions mesurée et théorique. Cette structure est utile pour la visualtion et les anovas. 

```{r melt-data}
data.silva <- data.silva %>% 
  melt(id.vars = c("databank", "nb_OTU", "dataset", "set_number", "amplicon", "abundance_law", "method"), 
       value.name = "divergence", 
       variable.name = "rank")
head(data.silva)
```

On va aussi construire un autre tableau qui intégre le fait que frogs constitue notre méthode de référence et qu'on compare, sur chaque échantillon, une méthode concurrente à la méthode de référence. 
```{r cast-data}
data.silva.2 <- data.silva %>% dcast(databank + nb_OTU + dataset + set_number + amplicon + abundance_law + rank ~ method) %>% 
  melt(id.vars = c("databank", "nb_OTU", "dataset", "set_number", "amplicon", "abundance_law", "rank", "frogs"), 
       value.name = "divergence", 
       variable.name = "method")
head(data.silva.2)
```
Dans data.silva.2, *divergence* correspond à la divergence mesurée sur une méthode concurrente (renseignée dans *method*) tandis que *frogs* correspond à la divergence mesurée par FROGS. 


## Visualisation des données

Avant de s'attaquer aux tests, on va comparer visuellement les niveaux de divergence obtenus. Pour insister sur la structure pairée des données (les divergences sont mesurées sur les **mêmes échantillons**) et sur le fait que Frogs est la méthode de référence, on va comparer les taux de divergences de Mothur à Frogs et de Uparse à Frogs. 

```{r plot-data-raw, fig.width=10, fig.height=7, message=FALSE}
p <- ggplot(mapping = aes(x = frogs, y = divergence, color = abundance_law, shape = amplicon)) + facet_grid(rank ~ nb_OTU) + geom_abline(slope = 1, intercept = 0) + theme_bw() + scale_x_log10() + scale_y_log10()
p.mothur <- p + geom_point(data = filter(data.silva.2, method == "mothur")) + labs(x = "divergence frogs", y = "divergence mothur")
p.uparse <- p + geom_point(data = filter(data.silva.2, method == "uparse")) + labs(x = "divergence frogs", y = "divergence uparse")
grid_arrange_shared_legend(p.mothur, p.uparse, ncol = 2)
```

On remarque que l'immense majorité des échantillons sont au dessus ou proche de la première bissectrice (droite y = x), ce qui signifie que FROGS a des performances meilleures ou comparables à ses deux concurrents. 

On peut aussi adopter une représentation plus classique des résultats à base de boxplot
```{r plot-data-raw-boxplot, fig.width=10, fig.height=7, warning=FALSE}
p <- ggplot(data.silva, mapping = aes(x = interaction(amplicon, abundance_law), y = divergence, color = method)) + facet_grid(rank ~ nb_OTU) + geom_boxplot() + theme_bw() + scale_y_continuous(limits = c(0, 20)) + theme(axis.text.x = element_text(angle = 90))
plot(p)
```

# Comparaison des méthodes

Le but de l'analyse est de comparer FROGS (considéré comme méthode de référence) à mothur et à uparse. On veut évaluer les performances de FROGS dans différentes régions de l'espace des paramètres définies par (1) un nombre d'OTUs, (2) une distribution d'abondances, (3) une région du 16S et (4) un rang taxonomique. Pour chacune de ces combinaisons, on a généré 10 communautés théorique différentes (*dataset*, qui reproduisent la variabilité biologique) et pour chaque communauté théorique, on a généré 10 jeux de lectures (*set_number*, qui reproduisent la variabilité technique). 

L'unité d'intéret la plus fine est la communauté théorique. Pour chaque communauté théorique, on veut savoir si FROGS reconstruit *mieux* la communauté que les méthodes concurrentes. Les réplicats techniques permettent de déterminer si la reconstruction de FROGS est *significativement* meilleure (ou pire) que celles de ses concurrentes. En aggrégant les résultats sur les communautés théoriques, on pourra déterminer dans quelles régions de l'espace des paramètres FROGS fait mieux, moins bien ou pareil que les méthodes concurrentes. 

## Tests pairés

### Test de student pairé 

Le test adopté (au niveau des réplicats techniques d'une communauté biologique) est un test de Student pairé (les divergences sur mesurées sur les mêmes échantillons). On peut aussi envisager sa contrepartie non-paramètrique: un test de Mann-Whitney contre l'hypothèse nulle $\mu = 0$ (pas de différence significative de divergence entre les méthodes). Cette étude est reportée à la section suivante. 

```{r batch-t-test}
silva.t.test <- data.silva.2 %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank, method) %>% 
  summarize(pval = t.test(frogs, divergence, paired = TRUE)$p.value, 
            measure = t.test(frogs, divergence, paired = TRUE)$estimate) %>% 
  mutate(best = ifelse(measure >0, "competitor", "frogs")) %>% 
  mutate(best = ifelse(pval < 0.05, best, "tied")) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
silva.t.test %>% select(measure, pval, best) %>% head()
```

Dans le tableau précédent, la colonne *mesure* représente l'écart de divergence moyenn entre frogs et son compétiteur (pour une communauté théorique donnée), *pval* correspond à la p-valeur associée à cette écart et *best* indique la meilleure des deux méthodes (*frogs*, *competitor* ou *tied* si on ne peut pas les distinguer).

Il ne reste plus qu'à représenter les données pour savoir dans quelles circonstances frogs fait mieux que la compétition.
```{r batch-t-test-plot, fig.width=10, fig.height=7}
frogs.color <- rgb(0, 1, 0, alpha = 0.6, maxColorValue = 1)
competitor.color <- rgb(1, 0, 0, alpha = 0.6, maxColorValue = 1)
p <- ggplot(mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = setNames(c(frogs.color, "grey60", competitor.color), c("frogs", "tied", "competitor"))) + theme(axis.text.x = element_text(angle = 90))
p.mothur <- p + geom_bar(data = filter(silva.t.test, method == "mothur")) + ylab("Number of communities (Mothur)")
p.uparse <- p + geom_bar(data = filter(silva.t.test, method == "uparse")) + ylab("Number of communities (Uparse)")
grid_arrange_shared_legend(p.mothur, p.uparse, ncol = 2)
```

Les graphiques montrent bien que dans la majorité des cas, FROGS est significativement meilleur ou équivalent à la méthode concurrente. FROGS fait mieux que la concurrence aux niveaux taxonomiques profonds (Species, Genus), en présence de beaucoup d'espèces et avec des distributions d'abondance uniformes. 

La seule situation dans laquelle FROGS fait pire que la concurrence est contre Uparse dans des communauté avec peu d'espèces (20). C'est probablement le reflet des filtres aggressifs de Uparse. 


### Test pairé non-paramétrique 

On adopte ici la même approche mais avec un test non paramétrique: un test de Mann-Whitney contre l'hypothèse nulle $\mu = 0$ (pas de différence significative de divergence entre les méthodes).  

```{r batch-wilcox-test-np}
silva.wilcox.test <- data.silva.2 %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank, method) %>% 
  summarize(pval = wilcox.test(frogs, divergence, paired = TRUE)$p.value, 
            measure = wilcox.test(frogs, divergence, paired = TRUE, conf.int = TRUE)$estimate) %>% 
  mutate(best = ifelse(measure >0, "competitor", "frogs")) %>% 
  mutate(best = ifelse(pval < 0.05, best, "tied")) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
silva.wilcox.test %>% select(measure, pval, best) %>% head()
```

Il ne reste plus qu'à représenter les données pour savoir dans quelles circonstances frogs fait mieux que la compétition.
```{r batch-wilcox-test-plot, fig.width=10, , fig.height=7}
frogs.color <- rgb(0, 1, 0, alpha = 0.6, maxColorValue = 1)
competitor.color <- rgb(1, 0, 0, alpha = 0.6, maxColorValue = 1)
p <- ggplot(mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = setNames(c(frogs.color, "grey60", competitor.color), c("frogs", "tied", "competitor"))) + theme(axis.text.x = element_text(angle = 90))
p.mothur <- p + geom_bar(data = filter(silva.wilcox.test, method == "mothur")) + ylab("Number of communities (Mothur)")
p.uparse <- p + geom_bar(data = filter(silva.wilcox.test, method == "uparse")) + ylab("Number of communities (Uparse)")
grid_arrange_shared_legend(p.mothur, p.uparse, ncol = 2)
```

avec les mêmes résultats que précédemment. 

## Tests non pairés

On commence par définir une fonction locale qui détermine (pour une communauté théorique) si frogs est la meilleure méthode ("frogs"), si elle est meilleure ex-aequo ("tied") ou une si une autre méthode fait strictement mieux ("other"). Les différences par rapport aux tests pairées sont (1) qu'on compare frogs à ses deux compétiteurs en même temps plutôt que de faire une comparaison par méthode et (2) que les tests ne sont plus pairés (on n'utilise pas l'information que les divergences sont mesurées sur les mêmes échantillons). Techniquement, il s'agit de tests multiples avec des contrastes contre une modalité de référence (ici frogs). La différence est basée sur le type de tests: ANOVA avec test post-hoc de Dunnett pour la version paramétrique, test de Kruskall-Wallis avec test post-hoc de Dunn pour la version non paramétrique (basée sur les rangs).

### Version paramétrique: Anova et test post-hoc de Dunnett

```{r determine-best}
determine_best <- function(x, y) {
  local.df <- data.frame(method = x, divergence = y)
  model <- lm(divergence ~ method, data = local.df)
  results <- fortify(summary(glht(model, linfct = mcp(method = "Dunnett"))))
  if (all(results$estimate >= 0 & results$p < 0.05)) {
    return("frogs")
  } else {
    better.method <- subset(results, p < 0.05 & estimate < 0)
    if (nrow(better.method)) {
      return("competitor")
    } else {
      return("tied")
    }
  }
}
```

avant d'appliquer cette fonction à chaque communauté théorique
```{r batch-anova}
silva.anova.test <- data.silva %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank) %>% 
  summarize(best = determine_best(method, divergence)) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
silva.anova.test %>% head()
```

On peut ensuite adopter la même représentation que précédemment
```{r batch-anova-plot, fig.width=10, fig.height=7}
frogs.color <- rgb(0, 1, 0, alpha = 0.6, maxColorValue = 1)
competitor.color <- rgb(1, 0, 0, alpha = 0.6, maxColorValue = 1)
p <- ggplot(silva.anova.test, mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = setNames(c(frogs.color, "grey60", competitor.color), c("frogs", "tied", "competitor"))) + theme(axis.text.x = element_text(angle = 90))
p <- p + geom_bar() + ylab("Number of communities")
plot(p)
```
Pour un petit nombre d'OTUs, FROGS fait mieux ou armes égales avec ses concurrents. Pour un nombre d'OTUs plus conséquent (à partir de 200), il commence à mieux marquer la différence. 

### Version non paramétrique: Kruskall-Wallis et test post-hoc de Dunn

Comme dans le cas paramétrique, on commence par définir une fonction locale qui détermine (pour une communauté théorique) si frogs est la meilleure méthode ("frogs"), si elle est meilleure ex-aequo ("tied") ou une si une autre méthode fait strictement mieux ("other"). La comparaison est basée sur un test de Dunn en utilisant frogs comme la modalité de contrôle. 


```{r determine-best-np}
determine_best_np <- function(x, g, method = "BH") {
  ## set frogs as reference factor
  g <- relevel(g, ref = "frogs")
  dunn.results <- dunn.test.control(x, g, p.adjust = method)
  results <- data.frame(method = rownames(dunn.results$p.value), 
                        p = as.numeric(dunn.results$p.value), 
                        estimate = as.numeric(dunn.results$statistic))
  if (all(results$estimate >= 0 & results$p < 0.05)) {
    return("frogs")
  } else {
    better.method <- subset(results, p < 0.05 & estimate < 0)
    if (nrow(better.method)) {
      return("competitor")
    } else {
      return("tied")
    }
  }
}
```

Le résultats est du même acabit que précédemment, on peut s'en assurer sur un jeu de données test:
```{r unit-test-determine-best-np}
test.data <- filter(data.silva, nb_OTU == 20 & dataset == "dataset_1" & amplicon == "V3V4" & abundance_law == "powerlaw" & rank == "Species")
determine_best_np(x = test.data$divergence, g = test.data$method)
```
avant d'appliquer cette fonction à chaque communauté théorique
```{r batch-dunn-test}
silva.dunn.test <- data.silva %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank) %>% 
  mutate(method = factor(method)) %>% 
  summarize(best = determine_best_np(divergence, method)) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
silva.dunn.test %>% head()
```

On peut ensuite adopter la même représentation que précédemment
```{r batch-dunn-plot, fig.width=10, fig.height=7}
frogs.color <- rgb(0, 1, 0, alpha = 0.6, maxColorValue = 1)
competitor.color <- rgb(1, 0, 0, alpha = 0.6, maxColorValue = 1)
p <- ggplot(silva.dunn.test, mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = setNames(c(frogs.color, "grey60", competitor.color), c("frogs", "tied", "competitor"))) + theme(axis.text.x = element_text(angle = 90))
p <- p + geom_bar() + ylab("Number of communities")
plot(p)
```
Pour un petit nombre d'OTUs, FROGS fait mieux ou armes égales avec ses concurrents. Pour un nombre d'OTUs plus conséquent (à partir de 200), il commence à mieux marquer la différence.
