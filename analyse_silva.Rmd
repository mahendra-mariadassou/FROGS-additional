---
title: "Comparaison FROGS/MOTHUR/UPARSE/QIIME sur la banque Silva"
author: "Mahendra Mariadassou, Géraldine Pascal"
date: "30 novembre 2016"
output:
  html_document:
    number_sections: yes
    theme: united
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true    
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
# setwd("~/Research_Projects/Microbiota/Geraldine/")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE,
                      fig.path = "Figures_20161130/Fig-silva/", 
                      cache.path = "cache/silva/")
```

# Introduction 

Ce document R markdown a vocation à documenter la procédure suivie pour comparer les résultats de FROGS, Mothur et UParse sur les jeux de données simulées. En particulier, on cherche à savoir si Frogs permet d'avoir une vision taxonomique des communautés bactériennes plus fidèle que Mothur et UParse. 

# Import et préparation des données

## Chargement des packages

On commence par charger quelques packages utiles:

- reshape et dplyr pour la manipulation de données
- ggplot2, grid et gridExtra pour la visualisation
- PMCMR pour effectuer des tests de Dunn (version non paramétrique du test post-hoc de Tukey)

```{r load-packages, message=FALSE}
library(ggplot2)
library(grid)
library(gridExtra)
library(reshape2)
library(PMCMR) ## The Pairwise Multiple Comparison of Mean Ranks Package (PMCMR)
library(multcomp)
library(dplyr)
```

et des fonction utilitaires pour combiner plusieurs graphiques ggplot et une palette de couleur customs:
```{r utility-functions, cache = FALSE}
source("utilities.R")
```

## Import des données

On importe ensuite les données
```{r import-data}
data <- read.table("silva.tsv", sep = "\t", header = TRUE)
head(data)
```

## Transformation des données

Les différentes réponses d'intéret sont `r names(data)[-c(1:7)]` stockées sur plusieurs colonnes. Avant toute chose, on change la variale nb_OTU en une variable numérique (pour retrouver un ordonnancement naturel) puis on sépare les informations sur le nombre de faux positifs et de faux négatifs dans un tableau distinct. 

```{r change-nb_OTU, warning = FALSE}
data$nb_OTU <- as.numeric(sub("sp", "", data$nb_OTU))
data.otus <- data %>% select(-one_of("Species", "Genus", "Family", "Order", "Class", "Phylum"))
```


Dans notre contexte on cherche à comparer les performances de mothur, silva et qiime à celles de frogs en termes de divergence à plusieurs niveaux taxonomiques. On va donc réorganiser le tableau en considérant que chaque "unité expérimentale"" est un jeu de données décrit à un niveau taxonomique donné par une méthode donnée et qu'on mesure dessus la divergence entre compositions mesurée et théorique. Cette structure est utile pour la visualtion et les anovas. 

```{r melt-data}
data <- data %>% select(-one_of("OTU.lost", "OTU.false")) %>%
  melt(id.vars = c("databank", "nb_OTU", "dataset", "set_number", "amplicon", "abundance_law", "method"), 
       value.name = "divergence", 
       variable.name = "rank")
head(data)
```

On va aussi construire un autre tableau qui intégre le fait que frogs constitue notre méthode de référence et qu'on compare, sur chaque échantillon, une méthode concurrente à la méthode de référence. 
```{r cast-data, message=FALSE}
data.2 <- data %>% dcast(databank + nb_OTU + dataset + set_number + amplicon + abundance_law + rank ~ method) %>% 
  melt(id.vars = c("databank", "nb_OTU", "dataset", "set_number", "amplicon", "abundance_law", "rank", "frogs"), 
       value.name = "divergence", 
       variable.name = "method")
head(data.2)
```
Dans data.2, *divergence* correspond à la divergence mesurée sur une méthode concurrente (renseignée dans *method*) tandis que *frogs* correspond à la divergence mesurée par FROGS. 

Finalement on change l'ordre des méthodes pour que les panels soient dans le bon ordre dans les graphiques
```{r data-change-method-order}
data.2 <- data.2 %>% mutate(method = factor(method, levels = c("uparse", "mothur", "qiime")))
```


## Visualisation des données

### Divergence 

Avant de s'attaquer aux tests, on va comparer visuellement les niveaux de divergence obtenus. Pour insister sur la structure pairée des données (les divergences sont mesurées sur les **mêmes échantillons**) et sur le fait que Frogs est la méthode de référence, on va comparer les taux de divergences de Mothur à Frogs et de Uparse à Frogs. 

```{r plot-data-raw, fig.width=12, fig.height=10, message=FALSE}
p <- ggplot(data = data.2, mapping = aes(x = frogs, y = divergence, color = abundance_law, shape = amplicon)) + facet_grid(rank ~ nb_OTU) + geom_abline(slope = 1, intercept = 0) + theme_bw() + scale_x_log10() + scale_y_log10() + xlab("divergence frogs")
plot.list <- generate_pointplot(p, data = data.2, vbl="method", ylab = "Divergence competitor")
grid_arrange_shared_legend(plot.list, ncol = 2)
```

On remarque que l'immense majorité des échantillons sont au dessus ou proche de la première bissectrice (droite y = x), ce qui signifie que FROGS a des performances meilleures ou comparables à ses deux concurrents. 

On peut aussi adopter une représentation plus classique des résultats à base de boxplot. Cette dernière montre qu'une distribution uniforme conduit à de plus grandes divergences, que mothur et uparse ont l'air moins bon sur des communautés complexes et enfin que mothur produit beaucoup plus d'outliers que les autres. 
```{r plot-data-raw-boxplot, fig.width=10, fig.height=8, warning=FALSE}
# p <- ggplot(data, mapping = aes(x = method, y = divergence, fill = method, color = method)) + facet_grid(rank ~ nb_OTU, scales = "free_y") + theme_bw() + scale_y_continuous(limits = c(0, NA)) + theme(axis.text.x = element_text(angle = 90)) + scale_fill_manual(values = manual.palette, guide = guide_legend(byrow = TRUE)) + scale_color_manual(values = manual.palette, guide = "none") + xlab(NULL)
# plot.list <- generate_boxplot(p, data %>% mutate(subset = interaction(amplicon, abundance_law)), 
#                               vbl = "subset", ylab = "Divergence")
# grid_arrange_shared_legend(plot.list, ncol = 2)
p <- ggplot(data, mapping = aes(x = interaction(amplicon, abundance_law), y = divergence, fill = method, color = method)) + facet_grid(rank ~ nb_OTU, scales = "free_y") + theme_bw() + scale_y_continuous(limits = c(0, NA)) + theme(axis.text.x = element_text(angle = 90)) + scale_fill_manual(values = manual.palette, guide = guide_legend(byrow = TRUE)) + scale_color_manual(values = manual.palette, guide = "none") + xlab(NULL) +
  geom_boxplot(outlier.size = 0.8) + geom_boxplot(aes(color = NULL), outlier.color = "transparent")
plot(p)
```

On produit la même figure mais après enlevé les données de mothur pour filtrer les divergences trop élevées. Les valeurs de divergence sont très comparables entre les 3 méthodes mais frogs et qiime semblent faire mieux que uparse, notamment pour les communautés complexes. 
```{r plot-data-raw-boxplot-no-mothur, fig.width=10, fig.height=8, warning=FALSE}
p <- ggplot(data %>% filter(method != "mothur"), mapping = aes(x = interaction(amplicon, abundance_law), y = divergence, fill = method, color = method)) + facet_grid(rank ~ nb_OTU, scales = "free_y") + theme_bw() + scale_y_continuous(limits = c(0, NA)) + theme(axis.text.x = element_text(angle = 90)) + scale_fill_manual(values = manual.palette, guide = guide_legend(byrow = TRUE)) + scale_color_manual(values = manual.palette, guide = "none") + xlab(NULL) +
  geom_boxplot(outlier.size = 0.8) + geom_boxplot(aes(color = NULL), outlier.color = "transparent")
plot(p)
```

Enfin, on peut se concentrer sur les résultats de FROGS en termes de divergence. De façon attendue, le taux de divergence augmente avec la finesse du rang taxonomique. La divergence est également plus élevée pour la région V4V4 que pour la V3V4 et plus élevée pour une distribution uniforme que pour une distribution en loi de puissance. 
```{r plot-frogs-only-divergence, fig.width=10, fig.height=5, warning=FALSE}
p <- ggplot(filter(data, method == "frogs"), mapping = aes(x = rank, y = divergence, fill = method, alpha = amplicon)) +
  facet_grid(abundance_law ~ nb_OTU, scales = "fixed") + 
  geom_boxplot() + 
  theme_bw() + 
  scale_y_continuous(limits = c(0, 20)) + theme(axis.text.x = element_text(angle = 90)) + scale_fill_manual(values = manual.palette) + 
  scale_alpha_discrete(guide = guide_legend(override.aes = list(fill = "grey10")), range = c(0.2, 0.6)) + 
  xlab(NULL) + ggtitle("Divergence")
plot(p)
```

### Faux positifs, faux négatifs
On peut aussi considérer le nombre de faux positifs et faux négatifs produits par chacune des 5 méthodes. La région V3V4 produit plus de faux positifs et moins de faux négatifs que la région V4V4. On voit bien que Qiime a le même comportement (assez mauvais) pour les faux positifs que Mothur. 
```{r plot-fp-fn-boxplot, fig.width=9, fig.height=8, warning=FALSE, message = FALSE}
p <- ggplot(data.otus, mapping = aes(x = method, fill = method, alpha = amplicon)) + facet_grid(nb_OTU ~ abundance_law, scales = "free_y") + theme_bw() + scale_y_continuous(limits = c(0, NA)) + theme(axis.text.x = element_text(angle = 90)) + scale_fill_manual(values = manual.palette, guide = guide_legend(byrow = TRUE)) + scale_color_manual(values = manual.palette) + scale_alpha_discrete(guide = guide_legend(override.aes = list(fill = "grey10")), range = c(0.2, 0.6))
p.fn <- p + geom_boxplot(aes(y = -OTU.lost)) + ylab(NULL) + ggtitle("False Negative OTUs")
p.fp <- p + geom_boxplot(aes(y = OTU.false)) + ylab(NULL) + ggtitle("False Positive OTUs")
grid_arrange_shared_legend(p.fp, p.fn, ncol = 2)
```

On se limite donc a FROGS et uparse pour mieux les comparer. On voit clairement que FROGS a de meilleurs performances en termes de faux négatifs et des performances comparables en termes de faux positifs (meilleurs pour une distribution uniforme, moins bonnes pour une distribution loi de puissance). 

```{r plot-fp-fn-boxplot-no-mothur, fig.width=9, fig.height=8, warning=FALSE, message = FALSE}
p <- ggplot(filter(data.otus, ! (method %in% c("mothur", "qiime"))), mapping = aes(x = method, fill = method, alpha = amplicon)) + facet_grid(nb_OTU ~ abundance_law, scales = "free_y") + theme_bw() + scale_y_continuous(limits = c(0, NA)) + theme(axis.text.x = element_text()) + scale_fill_manual(values = manual.palette, guide = guide_legend(byrow = TRUE)) + scale_color_manual(values = manual.palette) + scale_alpha_discrete(guide = guide_legend(override.aes = list(fill = "grey10")), range = c(0.2, 0.6))
p.fn <- p + geom_boxplot(aes(y = -OTU.lost)) + ylab(NULL) + ggtitle("False Negative OTUs")
p.fp <- p + geom_boxplot(aes(y = OTU.false)) + ylab(NULL) + ggtitle("False Positive OTUs")
grid_arrange_shared_legend(p.fp, p.fn, ncol = 2)
```

# Comparaison des méthodes: divergence

Le but de l'analyse est de comparer FROGS (considéré comme méthode de référence) à mothur et à uparse. On veut évaluer les performances de FROGS dans différentes régions de l'espace des paramètres définies par (1) un nombre d'OTUs, (2) une distribution d'abondances, (3) une région du 16S et (4) un rang taxonomique. Pour chacune de ces combinaisons, on a généré 10 communautés théorique différentes (*dataset*, qui reproduisent la variabilité biologique) et pour chaque communauté théorique, on a généré 10 jeux de lectures (*set_number*, qui reproduisent la variabilité technique). 

L'unité d'intéret la plus fine est la communauté théorique. Pour chaque communauté théorique, on veut savoir si FROGS reconstruit *mieux* la communauté que les méthodes concurrentes. Les réplicats techniques permettent de déterminer si la reconstruction de FROGS est *significativement* meilleure (ou pire) que celles de ses concurrentes. En aggrégant les résultats sur les communautés théoriques, on pourra déterminer dans quelles régions de l'espace des paramètres FROGS fait mieux, moins bien ou pareil que les méthodes concurrentes. 

## Tests pairés

### Test de student pairé 

Le test adopté (au niveau des réplicats techniques d'une communauté biologique) est un test de Student pairé (les divergences sur mesurées sur les mêmes échantillons). On peut aussi envisager sa contrepartie non-paramètrique: un test de Mann-Whitney contre l'hypothèse nulle $\mu = 0$ (pas de différence significative de divergence entre les méthodes). Cette étude est reportée à la section suivante. 

```{r batch-t-test}
data.t.test <- data.2 %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank, method) %>% 
  summarize(pval = t.test(frogs, divergence, paired = TRUE)$p.value, 
            measure = t.test(frogs, divergence, paired = TRUE)$estimate) %>% 
  mutate(best = ifelse(measure >0, "competitor", "frogs")) %>% 
  mutate(best = ifelse(pval < 0.05, best, "tied")) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
data.t.test %>% select(measure, pval, best) %>% head()
```

Dans le tableau précédent, la colonne *mesure* représente l'écart de divergence moyenn entre frogs et son compétiteur (pour une communauté théorique donnée), *pval* correspond à la p-valeur associée à cette écart et *best* indique la meilleure des deux méthodes (*frogs*, *competitor* ou *tied* si on ne peut pas les distinguer).

Il ne reste plus qu'à représenter les données pour savoir dans quelles circonstances frogs fait mieux que la compétition.
```{r batch-t-test-plot, fig.width=10, fig.height=10}
p <- ggplot(mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = manual.palette) + theme(axis.text.x = element_text(angle = 90))
plot.list <- generate_barplot(p, data = data.t.test, vbl = "method", ylab = "Number of communities")
grid_arrange_shared_legend(plot.list, ncol = 2)
```

Les graphiques montrent bien que dans la majorité des cas, FROGS est significativement meilleur ou équivalent à la méthode concurrente. FROGS fait mieux que la concurrence aux niveaux taxonomiques profonds (Species, Genus), en présence de beaucoup d'espèces et avec des distributions d'abondance uniforme.

Les deux seules situation dans lesquelles FROGS fait pire que la concurrence sont:

- contre Uparse dans des communauté avec peu d'espèces (20), probablement en reflet des filtres aggressifs de Uparse;
- contre Qiime dans des communauté avec beaucoup d'espèces (> 200) et distribution uniforme, probablement en reflet de la forte sensibilité de Qiime. 


### Test pairé non-paramétrique 

On adopte ici la même approche mais avec un test non paramétrique: un test de Mann-Whitney contre l'hypothèse nulle $\mu = 0$ (pas de différence significative de divergence entre les méthodes).  

```{r batch-wilcox-test-np}
data.wilcox.test <- data.2 %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank, method) %>% 
  summarize(pval = wilcox.test(frogs, divergence, paired = TRUE)$p.value, 
            measure = wilcox.test(frogs, divergence, paired = TRUE, conf.int = TRUE)$estimate) %>% 
  mutate(best = ifelse(measure >0, "competitor", "frogs")) %>% 
  mutate(best = ifelse(pval < 0.05, best, "tied")) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
data.wilcox.test %>% select(measure, pval, best) %>% head()
```

Il ne reste plus qu'à représenter les données pour savoir dans quelles circonstances frogs fait mieux que la compétition.
```{r batch-wilcox-test-plot, fig.width=10, fig.height=10}
p <- ggplot(mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = manual.palette) + theme(axis.text.x = element_text(angle = 90))
plot.list <- generate_barplot(p, data = data.wilcox.test, vbl = "method", ylab = "Number of communities")
grid_arrange_shared_legend(plot.list, ncol = 2)
```

avec les mêmes résultats que précédemment. 

## Tests non pairés

On commence par définir une fonction locale qui détermine (pour une communauté théorique) si frogs est la meilleure méthode ("frogs"), si elle est meilleure ex-aequo ("tied") ou une si une autre méthode fait strictement mieux ("other"). Les différences par rapport aux tests pairées sont (1) qu'on compare frogs à tous ses compétiteurs en même temps plutôt que de faire une comparaison par méthode et (2) que les tests ne sont plus pairés (on n'utilise pas l'information que les divergences sont mesurées sur les mêmes échantillons). Techniquement, il s'agit de tests multiples avec des contrastes contre une modalité de référence (ici frogs). La différence est basée sur le type de tests: ANOVA avec test post-hoc de Dunnett pour la version paramétrique, test de Kruskall-Wallis avec test post-hoc de Dunn pour la version non paramétrique (basée sur les rangs).

### Version paramétrique: Anova et test post-hoc de Dunnett

On applique la fonction determine_best à chaque communauté théorique pour déterminer qui de frogs et du compétiteur est meilleur que l'autre. 
```{r batch-anova}
data.anova.test <- data %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank) %>% 
  summarize(best = determine_best(method, divergence)) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
data.anova.test %>% head()
```

On peut ensuite adopter la même représentation que précédemment
```{r batch-anova-plot, fig.width=10, fig.height=7}
p <- ggplot(data.anova.test, mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = manual.palette) + theme(axis.text.x = element_text(angle = 90))
p <- p + geom_bar() + ylab("Number of communities")
plot(p)
```
Pour un petit nombre d'OTUs, FROGS fait mieux ou armes égales avec ses concurrents. Pour un nombre d'OTUs plus conséquent (à partir de 200) et en distribution uniforme, il fait mieux que la concurrence sur le V3V4 et moins bien sur le V4V4. Dans ce dernier cas, c'est Qiime qui réalise les divergences les plus faibles. 

### Version non paramétrique: Kruskall-Wallis et test post-hoc de Dunn

Comme dans le cas paramétrique, on commence par définir une fonction locale (determine\_best\_np) qui détermine (pour une communauté théorique) si frogs est la meilleure méthode ("frogs"), si elle est meilleure ex-aequo ("tied") ou une si une autre méthode fait strictement mieux ("other"). La comparaison est basée sur un test de Dunn en utilisant frogs comme la modalité de contrôle. 

avant d'appliquer cette fonction à chaque communauté théorique
```{r batch-dunn-test, warning=FALSE}
data.dunn.test <- data %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, rank) %>% 
  mutate(method = factor(method)) %>% 
  summarize(best = determine_best_np(divergence, method)) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
data.dunn.test %>% head()
```

On peut ensuite adopter la même représentation que précédemment
```{r batch-dunn-plot, fig.width=10, fig.height=7}
p <- ggplot(data.dunn.test, mapping = aes(x = rank, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = manual.palette) + theme(axis.text.x = element_text(angle = 90))
p <- p + geom_bar() + ylab("Number of communities")
plot(p)
```
Dans tous les cas, FROGS fait mieux ou armes égales avec ses concurrents. Le plus grande nombre d'égalités observées ici correspond à la moindre puissance du test non-paramétrique par rapport à sa version paramétrique. 

# Comparaison des méthodes: faux positifs et faux négatifs

Pour comparer les faux positifs et faux négatifs, on va se contenter de faire des tests pairés (paramétriques et non paramétriques) en considérant frogs comme méthode de référence. On commence pour ce faire par formatter le tableau des données au format idéal:

```{r format-data-otus}
data.otus.2 <- data.otus %>% melt(id.vars = c("databank", "nb_OTU", "dataset", "set_number", "amplicon", "abundance_law", "method"), 
       value.name = "divergence", 
       variable.name = "error_type") %>% 
  dcast(databank + nb_OTU + dataset + set_number + amplicon + abundance_law + error_type ~ method) %>% 
  melt(id.vars = c("databank", "nb_OTU", "dataset", "set_number", 
                   "amplicon", "abundance_law", "error_type", "frogs"), 
       value.name = "divergence", 
       variable.name = "method") %>% 
  mutate(divergence = abs(divergence), 
         error_type = ifelse(error_type == "OTU.lost", "FN", "FP"))
```

Puis on change l'ordre des méthodes
```{r data-otus-change-order}
data.otus.2 <- data.otus.2 %>% mutate(method = factor(method, levels = c("uparse", "mothur", "qiime")))
```


## Tests pairés

### Test de student pairé 

Le test adopté (au niveau des réplicats techniques d'une communauté biologique) est un test de Student pairé (les divergences sur mesurées sur les mêmes échantillons). On peut aussi envisager sa contrepartie non-paramètrique: un test de Mann-Whitney contre l'hypothèse nulle $\mu = 0$ (pas de différence significative de divergence entre les méthodes). Cette étude est reportée à la section suivante. 

```{r batch-t-test-otus}
data.otus.t.test <- data.otus.2 %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, error_type, method) %>% 
  summarize(pval = my.paired.t.test(frogs, divergence)$p.value, 
            measure = my.paired.t.test(frogs, divergence)$estimate) %>% 
  mutate(best = ifelse(measure > 0, "competitor", "frogs")) %>% 
  mutate(best = ifelse(pval < 0.05, best, "tied")) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
data.otus.t.test %>% select(measure, pval, best) %>% head()
```

Dans le tableau précédent, la colonne *mesure* représente l'écart de divergence moyenn entre frogs et son compétiteur (pour une communauté théorique donnée), *pval* correspond à la p-valeur associée à cette écart et *best* indique la meilleure des deux méthodes (*frogs*, *competitor* ou *tied* si on ne peut pas les distinguer).

Il ne reste plus qu'à représenter les données pour savoir dans quelles circonstances frogs fait mieux que la compétition.
```{r batch-t-test-otus-plot, fig.width=10, fig.height=10}
p <- ggplot(mapping = aes(x = error_type, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = manual.palette) + theme(axis.text.x = element_text(angle = 90)) + xlab("Error Type")
plot.list <- generate_barplot(p, data = data.otus.t.test, vbl = "method", ylab = "Number of communities")
grid_arrange_shared_legend(plot.list, ncol = 2)
```

Les graphiques montrent bien que dans la majorité des cas, FROGS est significativement meilleur ou équivalent à la méthode concurrente. FROGS fait mieux que Mothur(normal ou SOP) dans tous les cas de figures et fait Uparse (normal ou SOP) dans beaucoup de situations. Uparse s'impose en termes faux positifs pour de petits jeux de données, dans des distributions de type de powerlaw et lors de l'étude du V3V4. En particulier pour des écosystèmes riches, FROGS fait strictement mieux ou pareil que Uparse. 


### Test pairé non-paramétrique 

On adopte ici la même approche mais avec un test non paramétrique: un test de Mann-Whitney contre l'hypothèse nulle $\mu = 0$ (pas de différence significative de divergence entre les méthodes).  

```{r batch-wilcox-test-np-otus, warning=FALSE}
data.otus.wilcox.test <- data.otus.2 %>% group_by(databank, nb_OTU, dataset, amplicon, abundance_law, error_type, method) %>% 
  summarize(pval = my.paired.wilcox.test(frogs, divergence)$p.value, 
            measure = sum(sign(frogs - divergence) * rank(abs(frogs - divergence)))) %>% 
  mutate(best = ifelse(measure >0, "competitor", "frogs")) %>% 
  mutate(best = ifelse(pval < 0.05, best, "tied")) %>%
  mutate(best = factor(best, levels = c("frogs", "tied", "competitor")))
data.otus.wilcox.test %>% select(measure, pval, best) %>% head()
```

Il ne reste plus qu'à représenter les données pour savoir dans quelles circonstances frogs fait mieux que la compétition.
```{r batch-wilcox-test-plot-otus, fig.width=10, fig.height=10}
p <- ggplot(mapping = aes(x = error_type, fill = best)) + facet_grid(abundance_law + amplicon ~ nb_OTU) + theme_bw() + scale_fill_manual(values = manual.palette) + theme(axis.text.x = element_text(angle = 90)) + xlab("Error Type")
plot.list <- generate_barplot(p, data = data.otus.wilcox.test, vbl = "method", ylab = "Number of communities")
grid_arrange_shared_legend(plot.list, ncol = 2)
```

avec les mêmes résultats que précédemment. 
